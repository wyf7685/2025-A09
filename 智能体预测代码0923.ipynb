{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52424fa-8014-434e-ba7a-6f889faae5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bulingzheng/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib #引入画图数据库\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import scipy.fftpack as fftpack\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.stats.stattools import durbin_watson  # DW检验\n",
    "from statsmodels.graphics.api import qqplot  # qq图\n",
    "from scipy import io, integrate, linalg, signal\n",
    "from scipy.sparse.linalg import eigs\n",
    "from scipy.integrate import odeint, quad\n",
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7b8b549-7570-43e9-8bb8-caeb31df6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transferdata_try():\n",
    "    # 读取CSV文件\n",
    "    # df = pd.read_csv(\"备品备件库存移动清单查询.csv\", encoding=\"utf-8\")\n",
    "    df = pd.read_csv(\"2018-2023年备件领用记录.csv\", encoding=\"utf-8\")\n",
    "    # df = pd.read_csv(\"杭州领用记录.csv\", encoding=\"utf-8\")\n",
    "    # df = pd.read_csv(\"宁波领用记录.csv\", encoding=\"utf-8\")\n",
    "    # 将日期列转换为日期时间格式\n",
    "    df['凭证日期'] = pd.to_datetime(df['凭证日期'], format='%Y%m%d')\n",
    "    # df['凭证日期'] = pd.to_datetime(df['凭证日期'], format='%Y/%m/%d')\n",
    "    # df['凭证日期'] = pd.to_datetime(df['凭证日期'], format='%Y-%m-%d')\n",
    "    # 提取年份和月份列\n",
    "    df['年份'] = df['凭证日期'].dt.year\n",
    "    df['月份'] = df['凭证日期'].dt.month\n",
    "    # print(df)\n",
    "    # # 提取领用科室中含有\"杭州\"的行\n",
    "    # hangzhou_df = df[df['领用科室'].str.contains('杭州')]\n",
    "    # # 提取领用科室中含有\"宁波\"的行\n",
    "    # ningbo_df = df[df['领用科室'].str.contains('宁波')]\n",
    "    # # 打印结果\n",
    "    # print(\"杭州领用记录：\")\n",
    "    # print(hangzhou_df)\n",
    "    # print(\"\\n宁波领用记录：\")\n",
    "    # print(ningbo_df)\n",
    "    # hangzhou_df.to_csv(\"杭州领用记录.csv\", index=False, encoding='utf-8')\n",
    "    # ningbo_df.to_csv(\"宁波领用记录.csv\", index=False, encoding='utf-8')\n",
    "    # 使用pivot_table函数将数据透视为所需的格式\n",
    "    df_pivot = df.pivot_table(index='物料号', columns=['年份', '月份'], values='数量', aggfunc='sum', fill_value=0)\n",
    "    # # 获取物料号和账册编号的对应关系，筛选帐册编号为[C]的记录，并且只取每个物料号的第一个帐册编号\n",
    "    # material_to_A = df[df['帐册编号'] == '[A]'][['物料号', '帐册编号']].drop_duplicates(\n",
    "    #     subset=['物料号']).reset_index(drop=True)\n",
    "    # material_to_B = df[df['帐册编号'] == '[B]'][['物料号', '帐册编号']].drop_duplicates(\n",
    "    #     subset=['物料号']).reset_index(drop=True)\n",
    "    # material_to_C = df[df['帐册编号'] == '[C]'][['物料号', '帐册编号']].drop_duplicates(\n",
    "    #     subset=['物料号']).reset_index(drop=True)\n",
    "    #\n",
    "    #\n",
    "    # print(material_to_A)\n",
    "    # print(material_to_B)\n",
    "    # print(material_to_C)\n",
    "    # # 将筛选后的数据保存到 CSV 文件中\n",
    "    # material_to_C.to_csv('帐册C数据_all.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    # 重置列索引，将年份和月份组合为列名\n",
    "    df_pivot.columns = [f\"{year}.{month:02}\" for (year, month) in df_pivot.columns]\n",
    "\n",
    "    # 将列名转换为DateTime对象\n",
    "    df_pivot.columns = pd.to_datetime(df_pivot.columns, format='%Y.%m')\n",
    "\n",
    "    # 按季度对数据进行汇总\n",
    "    df_quarters = df_pivot.resample('Q', axis=1).sum()\n",
    "    # 为每个季度列重命名\n",
    "    quarter_names = ['第1季度', '第2季度', '第3季度', '第4季度']\n",
    "\n",
    "    # 获取数据中实际存在的年份\n",
    "    years = df_quarters.columns.year.unique()\n",
    "    # 生成对应的季度名称\n",
    "    new_column_names = []\n",
    "    for year in years:\n",
    "        quarters_in_year = 3 if year == max(years) else 4  # 最后一年的季度数量可能不足四个\n",
    "        new_column_names.extend([f\"{year} {quarter_names[i]}\" for i in range(quarters_in_year)])\n",
    "\n",
    "\n",
    "    df_quarters.columns = new_column_names\n",
    "    # 在这里先删去2023第三季度，后续在process里就不删了\n",
    "    # 删除名为 \"2023 第3季度\" 的列\n",
    "    df_quarters = df_quarters.drop(columns=['2023 第3季度'])\n",
    "\n",
    "    # print(\"zheshi1:\",df_quarters.columns)\n",
    "    non_zero_demand_count = (df_quarters.iloc[:, 0:] != 0).sum(axis=1)\n",
    "    print('zhehi',non_zero_demand_count)\n",
    "    # print(df_pivot)\n",
    "    # 计算ADI并添加到最后一列\n",
    "    # 计算非零需求的总数\n",
    "    # non_zero_demand_count = (df_pivot.iloc[:, 0:] != 0).sum(axis=1)\n",
    "\n",
    "    # 计算每年的年度总和\n",
    "    for year in range(df['年份'].min(), df['年份'].max() + 1):\n",
    "        if year == 2023:\n",
    "            end_quarter = 2  # 2023年只有2个季度\n",
    "        else:\n",
    "            end_quarter = 4\n",
    "\n",
    "        columns_for_year = []\n",
    "        for quarter in range(1, end_quarter + 1):\n",
    "            columns_for_year.append(f\"{year} 第{quarter}季度\")\n",
    "\n",
    "        year_sum = df_quarters[columns_for_year].sum(axis=1)\n",
    "        year_mean = df_quarters[columns_for_year].mean(axis=1)\n",
    "        year_std = df_quarters[columns_for_year].std(axis=1)\n",
    "        year_by = year_std / year_mean\n",
    "\n",
    "        # 在每年末尾插入年度总和列\n",
    "        insert_position = df_quarters.columns.get_loc(f\"{year} 第{end_quarter}季度\") + 1\n",
    "        df_quarters.insert(insert_position, f\"{year}年度总和\", year_sum)\n",
    "        df_quarters.insert(insert_position + 1, f\"{year}年均值\", year_mean)\n",
    "        df_quarters.insert(insert_position + 2, f\"{year}年标准差\", year_std)\n",
    "        df_quarters.insert(insert_position + 3, f\"{year}年变异系数\", year_by)\n",
    "    # print(df_pivot)\n",
    "    exclude_cols = ['年度总和', '年均值', '年标准差', '年变异系数']\n",
    "    # filtered_cols = [col for col in df_pivot.columns if all(exclude not in col for exclude in exclude_cols)]\n",
    "    filtered_cols = [str(col) for col in df_quarters.columns if\n",
    "                     all(exclude not in str(col) for exclude in exclude_cols)]\n",
    "\n",
    "    all_years_mean = df_quarters[filtered_cols].mean(axis=1)\n",
    "    # print(all_years_mean)\n",
    "    all_years_var = df_quarters[filtered_cols].var(axis=1)\n",
    "    all_years_std = df_quarters[filtered_cols].std(axis=1)\n",
    "    # 计算所有年份的均值（不包括年度总和列）\n",
    "    # all_years_mean = df_quarters[[col for col in df_quarters.columns if '年度总和' not in col]].mean(axis=1)\n",
    "    # all_years_var = df_quarters[[col for col in df_quarters.columns if '年度总和' not in col]].var(axis=1)\n",
    "    # all_years_std = df_quarters[[col for col in df_quarters.columns if '年度总和' not in col]].std(axis=1)\n",
    "    df_quarters['所有年均值'] = all_years_mean\n",
    "    df_quarters['所有方差'] = all_years_var\n",
    "    df_quarters['所有标准差'] = all_years_std\n",
    "    df_quarters['所有年变异系数'] = (all_years_std / all_years_mean)\n",
    "    df_pivot = df_quarters\n",
    "    # 计算ADI并添加到最后一列\n",
    "    # 计算非零需求的总数\n",
    "\n",
    "    df_pivot['ADI'] = 22 / non_zero_demand_count\n",
    "\n",
    "    # 重置索引，使物料号成为列\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "\n",
    "    # 添加物料描述列\n",
    "    material_description = df[['物料号', '物料描述']].drop_duplicates(subset=['物料号']).set_index('物料号')\n",
    "    df_pivot = df_pivot.join(material_description, on='物料号')\n",
    "    # print(len(df_pivot))\n",
    "    # df_pivot.to_csv(\"all数据表.csv\", index=False, encoding='utf-8')\n",
    "    # 找出所有年均值为0的物料数量\n",
    "    zero_mean_count = (df_pivot['所有年均值'] == 0).sum()\n",
    "    print(f\"所有年均值为0的物料数量：{zero_mean_count}\")\n",
    "\n",
    "    # 创建\"间歇性数据\"子数据集\n",
    "    intermittent_data = df_pivot[(df_pivot['ADI'] > 1.32) & (abs(df_pivot['所有年变异系数']) <= 0.7) &\n",
    "    (df_pivot['所有年均值'] < 0)]\n",
    "\n",
    "    # 创建\"块状需求\"子数据集\n",
    "    block_demand_data = df_pivot[(df_pivot['ADI'] > 1.32) & (abs(df_pivot['所有年变异系数']) > 0.7) &\n",
    "    (df_pivot['所有年均值'] < 0)]\n",
    "    # 创建\"需求平稳\"子数据集\n",
    "    stable_demand_data = df_pivot[(df_pivot['ADI'] <= 1.32) & (abs(df_pivot['所有年变异系数']) <= 0.7) &\n",
    "    (df_pivot['所有年均值'] < 0)]\n",
    "    # 创建\"不稳定需求\"子数据集\n",
    "    unstable_demand_data = df_pivot[(df_pivot['ADI'] <= 1.32) & (abs(df_pivot['所有年变异系数']) > 0.7) &\n",
    "    (df_pivot['所有年均值'] < 0)]\n",
    "    #\n",
    "    # 打印\"间歇性数据\"子数据集\n",
    "    print(\"间歇性数据 (ADI > 1.32 且 CV² <= 0.49):\", \"数量:\", len(intermittent_data))\n",
    "    print(intermittent_data)\n",
    "\n",
    "    # 打印\"块状需求\"子数据集\n",
    "    print(\"块状需求 (ADI > 1.32 且 CV² > 0.49):\", \"数量:\", len(block_demand_data))\n",
    "    print(block_demand_data)\n",
    "    #\n",
    "    # 打印\"需求平稳\"子数据集\n",
    "    print(\"需求平稳 (ADI <= 1.32 且 CV² <= 0.49):\", \"数量:\", len(stable_demand_data))\n",
    "    print(stable_demand_data)\n",
    "    stable_demand_data.to_csv(\"平稳杭宁.csv\", index=False, encoding='utf-8')\n",
    "    # 提取稳定需求数据中的物料号\n",
    "    material_numbers_stable = stable_demand_data['物料号']\n",
    "    # # 使用 merge 函数，将两个数据框根据物料号进行合并，判断是否在 df_c 中\n",
    "    merged_df = pd.merge(material_numbers_stable, material_to_C[['物料号']], on='物料号', how='inner')\n",
    "    print(len(merged_df))\n",
    "    merged_df.to_csv('都包含帐册C数据.csv', index=False, encoding='utf-8')\n",
    "    # 提取需求平稳数据集中 ADI 等于 1 的数据\n",
    "    adi_equals_1_data = stable_demand_data[stable_demand_data['ADI'] == 1]\n",
    "    # # 打印 ADI 等于 1 的数据\n",
    "    print(\"需求平稳且 ADI = 1 的数据:\", \"数量:\", len(adi_equals_1_data))\n",
    "    print(adi_equals_1_data)\n",
    "    adi_equals_1_data.to_csv(\"稳定杭宁.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "    # 打印\"不稳定需求\"子数据集\n",
    "    print(\"不稳定需求 (ADI <= 1.32 且 CV² > 0.49):\", \"数量:\", len(unstable_demand_data))\n",
    "    print(unstable_demand_data)\n",
    "    # unstable_demand_data.to_csv(\"不平稳杭宁.csv\", index=False, encoding='utf-8')\n",
    "    # 提取需求不平稳数据集中 ADI 等于 1 的数据\n",
    "    adi_equals_1_data_2 = unstable_demand_data[unstable_demand_data['ADI'] == 1]\n",
    "    # # 打印 ADI 等于 1 的数据\n",
    "    print(\"不稳定需求且 ADI = 1 的数据:\", \"数量:\", len(adi_equals_1_data_2))\n",
    "    print(adi_equals_1_data_2)\n",
    "    # adi_equals_1_data_2.to_csv(\"不稳定杭宁.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "    intermittent_data.to_csv(\"间歇性需求_季度_all.csv\", index=False, encoding='utf-8')\n",
    "    block_demand_data.to_csv(\"块状需求_季度_all.csv\", index=False, encoding='utf-8')\n",
    "    unstable_demand_data.to_csv(\"不稳定需求_季度_all.csv\", index=False, encoding='utf-8')\n",
    "    stable_demand_data.to_csv(\"需求平稳_季度_杭州.csv\", index=False, encoding='utf-8')\n",
    "    unstable_demand_data.to_csv(\"不稳定需求_季度_杭州.csv\", index=False, encoding='utf-8')\n",
    "    print(df_pivot)\n",
    "    df_pivot.to_csv(\"季度数据_all.csv\", index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54976625-38fe-4f7b-a430-ac7ff487da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_try():\n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(\"不稳定杭宁.csv\", encoding=\"utf-8\")\n",
    "\n",
    "    #df = pd.read_csv(\"新不稳定需求_季度_ADI=1_hz.csv\", encoding=\"ANSI\")\n",
    "    #df = pd.read_csv(\"新需求平稳_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\n",
    "    # df = pd.read_csv(\"新不稳定需求_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\n",
    "\n",
    "    # print(df)\n",
    "    # 定义一个函数，用于将负数转换为正数\n",
    "    def make_positive(x):\n",
    "        if isinstance(x, (int, float)):\n",
    "            return abs(x)\n",
    "        else:\n",
    "            return x  # 保持其他类型不变\n",
    "\n",
    "    # 使用applymap方法应用函数，只影响数值列\n",
    "    df = df.applymap(lambda x: make_positive(x))\n",
    "\n",
    "    # 获取要删除的列名列表\n",
    "    # columns_to_delete = [col for col in df.columns if col.endswith(('均值', '标准差', '方差', '变异系数', '总和', 'ADI'))]\n",
    "    columns_to_delete = [col for col in df.columns if col.endswith(('均值', '标准差', '方差', '变异系数', '总和', 'ADI','物料描述'))]\n",
    "\n",
    "\n",
    "    # 使用 drop 方法删除这些列\n",
    "    df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "    # 更改日期格式\n",
    "    df.columns = [col.replace('.', '/') for col in df.columns]\n",
    "\n",
    "    # 获取列索引\n",
    "    columns = df.columns\n",
    "\n",
    "    # 重置列索引为默认整数索引\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 创建一个新行，将列索引作为值\n",
    "    new_row = pd.DataFrame([columns], columns=df.columns)\n",
    "\n",
    "    # 将新行添加到DataFrame的顶部\n",
    "    df = pd.concat([new_row, df], axis=0, ignore_index=True)\n",
    "\n",
    "    # 转置矩阵\n",
    "    df = df.set_index(df.columns[0]).transpose()\n",
    "\n",
    "    # 重置行索引为默认整数索引\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 删除行索引的列名\n",
    "    df = df.rename_axis(None, axis=1)\n",
    "\n",
    "    # 更换'物料号'列索引为'time'\n",
    "    df = df.rename(columns={'物料号': 'time'})\n",
    "\n",
    "    # # 将'time'列转换为时间格式\n",
    "    # df['time'] = pd.to_datetime(df['time'], format='%Y/%m')\n",
    "    # print(df)\n",
    "    # 删去存在0的列\n",
    "    # non_zero_columns = df.loc[:, (df != 0).all()]\n",
    "    # df = non_zero_columns\n",
    "\n",
    "    # print(df)\n",
    "    # df.to_csv(\"宁波厂全部备件季度数据.csv\", index=False, encoding='ANSI')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47bf49cf-ca53-4e71-8443-160fb1be821b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '不稳定杭宁.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m data_SMA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# 运行\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[43mSMA_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 95\u001b[0m, in \u001b[0;36mSMA_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m sma_mape_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m data_SMA:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# 运行 SMAforecast() 并计算 MAPE\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     mape_single, mape_wight, col \u001b[38;5;241m=\u001b[39m \u001b[43mSMAforecast_try\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# 取出对应的物料号\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     material_number \u001b[38;5;241m=\u001b[39m col[n]\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36mSMAforecast_try\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      5\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# 窗口期\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 数据预处理\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_try\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print(df.columns)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 读取列索引名\u001b[39;00m\n\u001b[1;32m     11\u001b[0m col \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m, in \u001b[0;36mpreprocess_try\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_try\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 读取CSV文件\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m不稳定杭宁.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#df = pd.read_csv(\"新不稳定需求_季度_ADI=1_hz.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#df = pd.read_csv(\"新需求平稳_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新不稳定需求_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# 定义一个函数，用于将负数转换为正数\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_positive\u001b[39m(x):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '不稳定杭宁.csv'"
     ]
    }
   ],
   "source": [
    "# 简单移动平均法预测\n",
    "def SMAforecast_try(n):\n",
    "    # 参数\n",
    "    # n = 6               # 读取的列索引序号\n",
    "    window_size = 3  # 窗口期\n",
    "\n",
    "    # 数据预处理\n",
    "    df = preprocess_try()\n",
    "    # print(df.columns)\n",
    "    # 读取列索引名\n",
    "    col = df.columns\n",
    "\n",
    "    # print(df.iloc[:,n])\n",
    "    # # 计算移动平均值\n",
    "    y = df.iloc[:, n].rolling(window=window_size).mean()\n",
    "    # y_2 = df.iloc[:, n].ewm(span=window_size).mean()\n",
    "    # 使用rolling窗口并应用自定义权重\n",
    "\n",
    "    # # 定义自定义权重\n",
    "    # custom_weights = [0.1, 0.3, 0.6]\n",
    "    # y_2 = df.iloc[:, n].rolling(window=window_size).apply(\n",
    "    #     lambda x: (x * custom_weights).sum())\n",
    "\n",
    "    # 定义一组待尝试的权重组合\n",
    "    weight_combinations = [\n",
    "        [0.1, 0.3, 0.6],\n",
    "        [0.2, 0.3, 0.5],\n",
    "        [0.2, 0.4, 0.4],\n",
    "        [0.3, 0.3, 0.4],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7],\n",
    "        # 可以添加更多的组合\n",
    "    ]\n",
    "\n",
    "    mape_wight = float('inf')  # 初始化最佳 MAPE 为无穷大\n",
    "    best_weights = None  # 初始化最佳权重组合为空\n",
    "    y_2_predict = None\n",
    "    for weights in weight_combinations:\n",
    "        # 使用当前权重组合计算移动加权平均\n",
    "        y_2 = df.iloc[:, n].rolling(window=window_size).apply(\n",
    "            lambda x: (x * weights).sum())\n",
    "        # 计算 MAPE\n",
    "        mape_wight_2 = np.mean(np.abs((y_2[2:-1].values - df.iloc[3:, n].values) / df.iloc[3:, n].values))\n",
    "        # 更新最佳 MAPE 和对应的权重组合\n",
    "        if mape_wight_2 < mape_wight:\n",
    "            mape_wight = mape_wight_2\n",
    "            best_weights = weights\n",
    "            y_2_predict =y_2\n",
    "    print(\"最佳权重组合:\", best_weights)\n",
    "    print(\"最佳 MAPE:\", mape_wight)\n",
    "    print(\"最佳 加权预测值:\", y_2_predict)\n",
    "\n",
    "    # y.fillna(method='ffill', inplace=True)  # 前向填充\n",
    "    # y.fillna(method='bfill', inplace=True)  # 后向填充\n",
    "    # y_2_predict.fillna(method='ffill', inplace=True)  # 前向填充\n",
    "    # y_2_predict.fillna(method='bfill', inplace=True)  # 后向填充\n",
    "    # # print(y, y_2, df)\n",
    "\n",
    "    # MAPE\n",
    "    mape_single = np.mean(np.abs((y[2:-1].values - df.iloc[3:, n].values) / df.iloc[3:, n].values))\n",
    "    # mape_wight = np.mean(np.abs((y_2[2:-1].values - df.iloc[3:, n].values) / df.iloc[3:, n].values))\n",
    "    # mae_single = mean_absolute_error(df.iloc[3:, n].values, y[2:-1].values)\n",
    "    # mae_wight = mean_absolute_error(df.iloc[3:, n].values, y_2[2:-1].values)\n",
    "\n",
    "    # print('预测值', y[2:-1].values, '加权',y_2_predict[2:-1].values)\n",
    "    # print('真实值', df.iloc[3:, n].values)\n",
    "    # print(y[2:-1].values - df.iloc[3:, n].values)\n",
    "    print('mape', mape_single)\n",
    "    print('mape加权', mape_wight)\n",
    "\n",
    "    # print(df['time'])\n",
    "\n",
    "    time_values = df['time'].values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['time'], df.iloc[:, n], label='原始数据',color='black')\n",
    "    plt.plot(time_values[1:], y[:-1], label=f'一次 ({window_size}天)', color='b')\n",
    "    plt.plot(time_values[1:], y_2_predict[:-1], label=f'加权 ({window_size}天)', color='r')\n",
    "    plt.xlabel('日期')\n",
    "    plt.ylabel('数值')\n",
    "    # plt.title(f'物料号：{col[n]}\\n一次移动平均 MAPE:{round(mape_single, 2)} \\n 加权移动平均法 MAPE:{round(mape_wight,2)}')\n",
    "    plt.title(f'物料号：{col[n]}\\n一次移动平均 MAPE:{round(mape_single, 2)}  \\n 加权移动平均法 MAPE:{round(mape_wight, 2)} ')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True,linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('SMA')\n",
    "    plt.show()\n",
    "\n",
    "    return mape_single, mape_wight, col\n",
    "def SMA_run():\n",
    "    # 创建一个空列表，用于存储 MAPE 值\n",
    "    sma_mape_values = []\n",
    "    for n in data_SMA:\n",
    "        # 运行 SMAforecast() 并计算 MAPE\n",
    "        mape_single, mape_wight, col = SMAforecast_try(n)\n",
    "        # 取出对应的物料号\n",
    "        material_number = col[n]\n",
    "        # 将 MAPE 值作为元组添加到列表中\n",
    "        sma_mape_values.append((col[n], mape_single, mape_wight))\n",
    "    # 将 MAPE 值写入文件\n",
    "    with open(\"SMA不平稳杭宁.txt\", \"w\") as f:\n",
    "        # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "        for material_number, mape_single, mape_wight in sma_mape_values:\n",
    "            f.write(\n",
    "                f\"物料号：{material_number}, MAPE Single: {round(mape_single, 2)}, MAPE Weight: {round(mape_wight, 2)}\\n\")\n",
    "    csv_filename = \"SMA不平稳杭宁.csv\"\n",
    "    with open(csv_filename, \"a\", newline='') as f:  # 使用追加模式，并且指定 newline='' 来避免空行\n",
    "        writer = csv.writer(f)\n",
    "        # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "        for material_number, mape_single, mape_wight in sma_mape_values:\n",
    "            writer.writerow([material_number, round(mape_single, 2), round(mape_wight, 2)])\n",
    "        plt.show()\n",
    "    return\n",
    "if __name__ == '__main__':\n",
    "    # 物料取值\n",
    "    data_SMA = range(1,2)\n",
    "    # 运行\n",
    "    SMA_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2828cbaf-57a6-45db-b71e-d97f95a750f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: ANSI",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 173\u001b[0m\n\u001b[1;32m    171\u001b[0m data_EMA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# 运行\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mEMA_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 147\u001b[0m, in \u001b[0;36mEMA_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m ema_mape_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m data_EMA:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# 运行 SMAforecast() 并计算 MAPE\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     best_mape_one, best_mape_two, best_mape_three, col, best_alpha_d, best_beta_d, best_alpha_t, best_beta_t, best_gamma_t, best_season_periods \u001b[38;5;241m=\u001b[39m \u001b[43mEMAforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# 取出对应的物料号\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     material_number \u001b[38;5;241m=\u001b[39m col[n]\n",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m, in \u001b[0;36mEMAforecast\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mEMAforecast\u001b[39m(n):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# 数据预处理\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_try\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 读取列索引名\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     col \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "Cell \u001b[0;32mIn[18], line 195\u001b[0m, in \u001b[0;36mpreprocess_try\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_try\u001b[39m():\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 读取CSV文件\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m不稳定杭宁.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANSI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新不稳定需求_季度_ADI=1_hz.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新需求平稳_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新不稳定需求_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# 定义一个函数，用于将负数转换为正数\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_positive\u001b[39m(x):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:723\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    720\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# validate encoding and errors\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(errors, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n",
      "\u001b[0;31mLookupError\u001b[0m: unknown encoding: ANSI"
     ]
    }
   ],
   "source": [
    "# 指数平滑预测(一、二、三)\n",
    "def EMAforecast(n):\n",
    "    # 数据预处理\n",
    "    df = preprocess_try()\n",
    "    # 读取列索引名\n",
    "    col = df.columns\n",
    "    # 转换为日期时间索引\n",
    "    df.set_index('time', inplace=True)\n",
    "    # 划分训练和测试\n",
    "    train = df[:'2022 第2季度']\n",
    "    test = df['2022 第3季度':]\n",
    "    train = train.iloc[:, n - 1]\n",
    "    train = train.astype(float)\n",
    "    test = test.iloc[:, n - 1]\n",
    "    test = test.astype(float)\n",
    "    # 定义一个指数平滑模型的超参数优化函数\n",
    "    # 遍历不同的 alpha 值，通过拟合一次指数平滑模型到训练数据，并在测试集上计算 MAE，找到最佳的 alpha 值，\n",
    "    # 然后返回最佳的 alpha 值和对应的 MAE 值。这样可以帮助选择在指数平滑模型中的最佳超参数，以获得最佳的预测性能。\n",
    "    def ses_optimizer(train, alphas, step=4):  # step for length of test set\n",
    "        # 初始化最佳的 alpha 值和最佳的 MAPE 为无穷大\n",
    "        best_alpha, best_mape_one, best_mae_one = None, float(\"inf\"), float(\"inf\")\n",
    "        # 遍历 alpha 值的候选列表\n",
    "        for alpha in alphas:\n",
    "            # 使用当前的 alpha 值拟合一次指数平滑模型到训练数据\n",
    "            ses_model = SimpleExpSmoothing(train).fit(smoothing_level=alpha)\n",
    "            # 使用拟合的模型预测未来 step 个时间点的值\n",
    "            y_pred = ses_model.forecast(step)\n",
    "            # 计算预测值与测试集的平均绝对误差（MAPE）\n",
    "            mape_one = np.mean(np.abs((y_pred.values - test.values) / test.values))\n",
    "            # 如果当前 alpha 值的 MAPE小于最佳 MAPE，则更新最佳 alpha 值和最佳 MAPE\n",
    "            if mape_one < best_mape_one:\n",
    "                best_alpha, best_mape_one = alpha, mape_one\n",
    "            # 打印当前 alpha 值和对应的 MAPE 值\n",
    "            print(\"alpha:\", round(alpha, 2), \"mape:\", round(mape_one, 4))\n",
    "        # 打印最佳的 alpha 值和对应的最佳 MAPE 值\n",
    "        print(\"best_alpha:\", round(best_alpha, 2), \"best_mape_one\", round(best_mape_one, 4))\n",
    "        # 返回最佳的 alpha 值和最佳的 MAPE值\n",
    "        return best_alpha, best_mape_one\n",
    "\n",
    "    # 定义一个二次指数平滑模型的超参数优化函数\n",
    "    # 用于选择最佳的 alpha 和 beta 参数\n",
    "    def des_optimizer(train, alphas, betas, step=4):\n",
    "        best_alpha, best_beta, best_mape_two, best_mae_two = None, None, float(\"inf\"), float(\"inf\")  # 初始化最佳参数和最佳MAPE\n",
    "        for alpha in alphas:  # 遍历alpha参数列表\n",
    "            for beta in betas:  # 遍历beta参数列表\n",
    "                # 使用ExponentialSmoothing模型拟合训练数据，设置趋势为\"add\"，并指定alpha和beta参数\n",
    "                des_model = ExponentialSmoothing(train, trend=\"add\").fit(smoothing_level=alpha, smoothing_slope=beta)\n",
    "                y_pred = des_model.forecast(step)  # 预测未来step个时间步长的值\n",
    "                # 计算预测值与测试集的平均绝对误差（MAPE）\n",
    "                mape_two = np.mean(np.abs((y_pred.values - test.values) / test.values))\n",
    "                if mape_two < best_mape_two:  # 如果当前MAPE更好（更小），则更新最佳参数和最佳MAPE\n",
    "                    best_alpha, best_beta, best_mape_two = alpha, beta, mape_two\n",
    "                # 打印当前参数组合的结果\n",
    "                print(\"alpha:\", round(alpha, 2), \"beta:\", round(beta, 2), \"mape:\", round(mape_two, 4))\n",
    "        # 打印最佳参数和对应的最佳MAE\n",
    "        print(\"best_alpha:\", round(best_alpha, 2), \"best_beta:\", round(best_beta, 2), \"best_mape:\",\n",
    "              round(best_mape_two, 4))\n",
    "        return best_alpha, best_beta, best_mape_two # 返回最佳参数和最佳MAPE\n",
    "\n",
    "    # 定义一个三次指数平滑模型的超参数优化函数\n",
    "    # 用于选择最佳的 alpha、beta、gamma 参数\n",
    "    def tes_optimizer(train, abg, step=4):\n",
    "        # 初始化最佳的 alpha、beta、gamma 和最佳 MAPE\n",
    "        best_alpha, best_beta, best_gamma, best_mape_three, best_season_periods = None, None, None, float(\"inf\"), float(\"inf\")\n",
    "        # 遍历参数组合列表\n",
    "        for comb in abg:\n",
    "            # 使用给定的参数组合构建三次指数平滑模型-加法模型\n",
    "            tes_model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=comb[3]). \\\n",
    "                fit(smoothing_level=comb[0], smoothing_slope=comb[1], smoothing_seasonal=comb[2])\n",
    "            # 使用模型预测未来的时间点\n",
    "            y_pred = tes_model.forecast(step)\n",
    "            mape_three = np.mean(np.abs((y_pred.values - test.values) / test.values))\n",
    "\n",
    "            # 如果当前 MAPE 更好（更小），则更新最佳参数和最佳 MAPE\n",
    "            if mape_three < best_mape_three:\n",
    "                best_alpha, best_beta, best_gamma, best_season_periods, best_mape_three = comb[0], comb[1], comb[2],comb[3] ,mape_three\n",
    "            # 打印当前参数组合的结果\n",
    "            print(\"alpha:\", round(comb[0], 2), \"beta:\", round(comb[1], 2), \"gamma:\", round(comb[2], 2), \"season:\",round(comb[3],2),\"mape:\",\n",
    "                  round(mape_three, 2))\n",
    "        # 打印最佳参数和对应的最佳 MAE\n",
    "        print(\"best_alpha:\", round(best_alpha, 2), \"best_beta:\", round(best_beta, 2), \"best_gamma:\",\n",
    "              round(best_gamma, 2),\"best_season:\",round(best_season_periods),\n",
    "              \"best_mape:\", round(best_mape_three, 4))\n",
    "        # 返回最佳的 alpha、beta、gamma 和最佳 MAPE\n",
    "        return best_alpha, best_beta, best_gamma, best_season_periods, best_mape_three\n",
    "\n",
    "    # 一次指数；包含了一系列从0.1到1.0的数字，间隔为0.1（更精确可选间隔为0.01）\n",
    "    alphas = np.arange(0.10, 1, 0.10)\n",
    "    # 二次指数\n",
    "    alphas_d = np.arange(0.10, 1, 0.10)\n",
    "    betas_d = np.arange(0.10, 1, 0.10)\n",
    "    # 三次指数   保存最优的参数值，寻找规律，缩小范围，寻找对应参数的普适性；找找最优参数的规律，比如：某个区间\n",
    "    alphas_t = betas_t = gammas_t = np.arange(0.10, 1, 0.10)\n",
    "    # 季节性周期\n",
    "    season_t = np.arange(2, 7, 1)\n",
    "    abg = list(itertools.product(alphas_t, betas_t, gammas_t, season_t))\n",
    "\n",
    "    # 保存值\n",
    "    best_alpha, best_mape_one = ses_optimizer(train, alphas)\n",
    "    best_alpha_d, best_beta_d, best_mape_two = des_optimizer(train, alphas_d, betas_d)\n",
    "    best_alpha_t, best_beta_t, best_gamma_t,best_season_periods, best_mape_three = tes_optimizer(train, abg)\n",
    "\n",
    "    # 一次指数平滑\n",
    "    ses_model = SimpleExpSmoothing(train).fit(smoothing_level=best_alpha)\n",
    "    # 二次指数平滑\n",
    "    des_model = ExponentialSmoothing(train, trend=\"add\").fit(smoothing_level=best_alpha_d, smoothing_slope=best_beta_d)\n",
    "    # 三次指数平滑Holt-Winters\n",
    "    tes_model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=best_season_periods). \\\n",
    "        fit(smoothing_level=best_alpha_t, smoothing_trend=best_beta_t, smoothing_seasonal=best_gamma_t)\n",
    "\n",
    "    # 4个预测值\n",
    "    y_pred = ses_model.forecast(4)\n",
    "    y_pred_d = des_model.forecast(4)\n",
    "    y_pred_t = tes_model.forecast(4)\n",
    "    print(\"真实值：\",test.values)\n",
    "    print(\"三次预测值：\",y_pred_t)\n",
    "\n",
    "    # 作图\n",
    "    def plot_forecast(train, test, y_pred, title):\n",
    "        mape = np.mean(np.abs((y_pred.values - test.values) / test.values))\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # 合并训练和测试数据\n",
    "        full_data = pd.concat([train, test])\n",
    "        plt.plot(full_data.index, full_data.values, label=' Training Data', color='black')  # 合并后的数据\n",
    "        plt.plot(test.index, y_pred, label='Forecast', color='red')\n",
    "        plt.title(f\"{df.columns[n - 1]}, {title}, MAPE:{round(mape, 2)}\")\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Values')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(title)\n",
    "        plt.show()\n",
    "\n",
    "    plot_forecast(train, test, y_pred, \"Single Exponential Smoothing\")\n",
    "    plot_forecast(train, test, y_pred_d, \"Double Exponential Smoothing\")\n",
    "    plot_forecast(train, test, y_pred_t, \"Triple Exponential Smoothing\")\n",
    "    return best_mape_one, best_mape_two, best_mape_three, col, best_alpha_d, best_beta_d, best_alpha_t, best_beta_t, best_gamma_t, best_season_periods\n",
    "\n",
    "# 运行指数平滑法、写入\n",
    "def EMA_run():\n",
    "    # 创建一个空列表，用于存储 MAPE 值\n",
    "    ema_mape_values = []\n",
    "    for n in data_EMA:\n",
    "        # 运行 SMAforecast() 并计算 MAPE\n",
    "        best_mape_one, best_mape_two, best_mape_three, col, best_alpha_d, best_beta_d, best_alpha_t, best_beta_t, best_gamma_t, best_season_periods = EMAforecast(n)\n",
    "        # 取出对应的物料号\n",
    "        material_number = col[n]\n",
    "        print(\"这是\", material_number)\n",
    "        print(best_mape_one, best_mape_two, best_mape_three, best_alpha_d, best_beta_d, best_alpha_t, best_beta_t, best_gamma_t, best_season_periods)\n",
    "        # 将 MAPE 值作为元组添加到列表中\n",
    "        ema_mape_values.append((material_number, best_mape_one, best_mape_two, best_mape_three, best_alpha_d, best_beta_d, best_alpha_t, best_beta_t, best_gamma_t, best_season_periods))\n",
    "    # # 将 MAPE 值写入文件\n",
    "    # with open(\"EMA平稳杭宁_4.txt\", \"w\") as f:\n",
    "    #     # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "    #     for material_number, best_mapex_one, best_mapex_two, best_mapex_three, best_alpha_d, best_beta_d, best_alpha_t, best_beta_t, best_gamma_t, best_season_periods in ema_mape_values:\n",
    "    #         f.write(\n",
    "    #             f\"物料号：{material_number}, MAPEX One: {round(best_mapex_one, 2)}, MAPE Two: {round(best_mapex_two, 2)},MAPE Three:{round(best_mapex_three, 2)},TWO alpha beta:{round(best_alpha_d, 2), round(best_beta_d, 2)},Three alpha beta gamma:{round(best_alpha_t, 2), round(best_beta_t, 2), round(best_gamma_t, 2)},season_prriods:{best_season_periods}\\n\")\n",
    "    # csv_filename = \"EMA平稳杭宁_4.csv\"\n",
    "    # with open(csv_filename, \"a\", newline='') as f:  # 使用追加模式，并且指定 newline='' 来避免空行\n",
    "    #     writer = csv.writer(f)\n",
    "    #     # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "    #     for material_number, best_mapex_one, best_mapex_two, best_mapex_three, best_alpha_d, best_beta_d, best_alpha_t, best_beta_t, best_gamma_t, best_season_periods in ema_mape_values:\n",
    "    #         writer.writerow(\n",
    "    #             [material_number, round(best_mapex_one, 2), round(best_mapex_two, 2), round(best_mapex_three, 2),round(best_alpha_d,2),round( best_beta_d,2),round(best_alpha_t,2),round(best_beta_t,2),round(best_gamma_t,2),round(best_season_periods,2)])\n",
    "    #     plt.show()\n",
    "    return\n",
    "if __name__ == '__main__':\n",
    "    # 物料取值\n",
    "    data_EMA = range(1,2)\n",
    "    # 运行\n",
    "    EMA_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9b9bb60-e9f8-43a6-b210-338d406a7e1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: ANSI",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 279\u001b[0m\n\u001b[1;32m    277\u001b[0m data_CRO \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# 运行\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m \u001b[43mCRO_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 253\u001b[0m, in \u001b[0;36mCRO_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m cro_mape_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m data_CRO:\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# 运行 SMAforecast_try() 并计算 MAPE\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     best_mape_cst, best_mape_sba, best_mape_tsb, col \u001b[38;5;241m=\u001b[39m \u001b[43mCroston_try\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# 取出对应的物料号\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     material_number \u001b[38;5;241m=\u001b[39m col[n]\n",
      "Cell \u001b[0;32mIn[16], line 200\u001b[0m, in \u001b[0;36mCroston_try\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_alpha_tsb, best_beta_tsb, best_mape_tsb\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# 调用\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_try\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# df = preprocess()\u001b[39;00m\n\u001b[1;32m    202\u001b[0m data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, n]\n",
      "Cell \u001b[0;32mIn[2], line 195\u001b[0m, in \u001b[0;36mpreprocess_try\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_try\u001b[39m():\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 读取CSV文件\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m不稳定杭宁.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANSI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新不稳定需求_季度_ADI=1_hz.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新需求平稳_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新不稳定需求_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# 定义一个函数，用于将负数转换为正数\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_positive\u001b[39m(x):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:723\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    720\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# validate encoding and errors\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(errors, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n",
      "\u001b[0;31mLookupError\u001b[0m: unknown encoding: ANSI"
     ]
    }
   ],
   "source": [
    "#croston预测\n",
    "def Croston_try(n):\n",
    "    # croston\n",
    "    def croston(ts, alpha, extra_periods=1):\n",
    "        # 将输入数据转换为NumPy数组\n",
    "        d = np.array(ts)\n",
    "        # 获取历史期间的长度\n",
    "        cols = len(d)\n",
    "        # 在需求数组中附加np.nan，以覆盖未来期间\n",
    "        d = np.append(d, [np.nan] * extra_periods)\n",
    "\n",
    "        # 初始化水平（a）、周期性（p）和预测（f）\n",
    "        a, p, f = np.full((3, cols + extra_periods), np.nan)\n",
    "        q = 1  # 自上次需求观察以来的期数\n",
    "\n",
    "        # 初始化\n",
    "        # 找到第一个需求观察值的位置\n",
    "        first_occurrence = np.argmax(d[:cols] > 0)\n",
    "        a[0] = d[first_occurrence]\n",
    "        p[0] = 1 + first_occurrence\n",
    "        f[0] = a[0] / p[0]\n",
    "\n",
    "        # 创建所有t+1时刻的预测\n",
    "        for t in range(0, cols):\n",
    "            if d[t] > 0:\n",
    "                a[t + 1] = alpha * d[t] + (1 - alpha) * a[t]\n",
    "                p[t + 1] = alpha * q + (1 - alpha) * p[t]\n",
    "                f[t + 1] = a[t + 1] / p[t + 1]\n",
    "                q = 1\n",
    "            else:\n",
    "                a[t + 1] = a[t]\n",
    "                p[t + 1] = p[t]\n",
    "                f[t + 1] = f[t]\n",
    "                q += 1\n",
    "\n",
    "        # 预测未来期间\n",
    "        a[cols + 1: cols + extra_periods] = a[cols]\n",
    "        p[cols + 1: cols + extra_periods] = p[cols]\n",
    "        f[cols + 1: cols + extra_periods] = f[cols]\n",
    "\n",
    "        # 创建一个包含需求、预测、周期、水平和误差的Pandas DataFrame\n",
    "        df_CR = pd.DataFrame.from_dict({\"Demand\": d, \"Forecast\": f, \"Period\": p, \"Level\": a, \"Error\": d - f})\n",
    "\n",
    "        return df_CR\n",
    "\n",
    "    # croston_sba\n",
    "    def croston_SBA(ts, alpha, extra_periods=1):\n",
    "        # 将输入数据转换为NumPy数组\n",
    "        d = np.array(ts)\n",
    "        # 获取历史期间的长度\n",
    "        cols = len(d)\n",
    "        # 在需求数组中附加np.nan，以覆盖未来期间\n",
    "        d = np.append(d, [np.nan] * extra_periods)\n",
    "\n",
    "        # 初始化水平（a）、周期性（p）和预测（f）\n",
    "        a, p, f = np.full((3, cols + extra_periods), np.nan)\n",
    "        q = 1  # 自上次需求观察以来的期数\n",
    "\n",
    "        # 初始化\n",
    "        # 找到第一个需求观察值的位置\n",
    "        first_occurrence = np.argmax(d[:cols] > 0)\n",
    "        a[0] = d[first_occurrence]\n",
    "        p[0] = 1 + first_occurrence\n",
    "        f[0] = a[0] / p[0]\n",
    "\n",
    "        # 创建所有t+1时刻的预测\n",
    "        for t in range(0, cols):\n",
    "            if d[t] > 0:\n",
    "                a[t + 1] = alpha * d[t] + (1 - alpha) * a[t]\n",
    "                p[t + 1] = alpha * q + (1 - alpha) * p[t]\n",
    "                f[t + 1] = (1 - alpha / (2)) * (a[t + 1] / p[t + 1])\n",
    "                q = 1\n",
    "            else:\n",
    "                a[t + 1] = a[t]\n",
    "                p[t + 1] = p[t]\n",
    "                f[t + 1] = f[t]\n",
    "                q += 1\n",
    "\n",
    "        # 预测未来期间\n",
    "        a[cols + 1: cols + extra_periods] = a[cols]\n",
    "        p[cols + 1: cols + extra_periods] = p[cols]\n",
    "        f[cols + 1: cols + extra_periods] = f[cols]\n",
    "\n",
    "        # 创建一个包含需求、预测、周期、水平和误差的Pandas DataFrame\n",
    "        df_SBA = pd.DataFrame.from_dict({\"Demand\": d, \"Forecast\": f, \"Period\": p, \"Level\": a, \"Error\": d - f})\n",
    "\n",
    "        return df_SBA\n",
    "\n",
    "    # croston_tsb\n",
    "    def croston_tsb(ts, alpha, beta, extra_periods=1):\n",
    "        d = np.array(ts)  # Transform the input into a numpy array\n",
    "        cols = len(d)  # Historical period length\n",
    "        d = np.append(d, [np.nan] * extra_periods)  # Append np.nan into the demand array to cover future periods\n",
    "\n",
    "        # Level (a), probability (p), and forecast (f)\n",
    "        a, p, f = np.full((3, cols + extra_periods), np.nan)\n",
    "\n",
    "        # Initialization\n",
    "        first_occurrence = np.argmax(d[:cols] > 0)\n",
    "        a[0] = d[first_occurrence]\n",
    "        p[0] = 1 / (1 + first_occurrence)\n",
    "        f[0] = p[0] * a[0]\n",
    "\n",
    "        # Create all the t+1 forecasts\n",
    "        for t in range(0, cols):\n",
    "            if d[t] > 0:\n",
    "                a[t + 1] = alpha * d[t] + (1 - alpha) * a[t]\n",
    "                p[t + 1] = beta * (1) + (1 - beta) * p[t]\n",
    "            else:\n",
    "                a[t + 1] = a[t]\n",
    "                p[t + 1] = (1 - beta) * p[t]\n",
    "            f[t + 1] = p[t + 1] * a[t + 1]\n",
    "\n",
    "        # Future Forecast\n",
    "        a[cols + 1: cols + extra_periods] = a[cols]\n",
    "        p[cols + 1: cols + extra_periods] = p[cols]\n",
    "        f[cols + 1: cols + extra_periods] = f[cols]\n",
    "\n",
    "        df_TSB = pd.DataFrame.from_dict({\"Demand\": d, \"Forecast\": f, \"Period\": p, \"Level\": a, \"Error\": d - f})\n",
    "        return df_TSB\n",
    "\n",
    "    # croston参数迭代\n",
    "    def cst_optimizer(data):\n",
    "        best_mape_cst = 999\n",
    "        # best_mae_cst = 999\n",
    "        alphas = np.arange(0, 1, 0.1)\n",
    "        i = 0\n",
    "        for alpha in alphas:\n",
    "            i = i + 1\n",
    "            result_cr = croston(data, alpha)\n",
    "            forecast_cst = result_cr['Forecast']\n",
    "\n",
    "            mape_cst = np.mean(np.abs((forecast_cst[:-1].values - data.values) / data.values))\n",
    "            # mae_cst = mean_absolute_error(data.values, forecast_cst[:-1].values)\n",
    "            if mape_cst < best_mape_cst:\n",
    "                best_alpha_cst, best_mape_cst = alpha, mape_cst\n",
    "            print('cst第', i, '次迭代')\n",
    "            print(\"alpha:\", round(alpha, 2), \"mape:\", round(mape_cst, 4))\n",
    "            # print(\"alpha:\", round(alpha, 2), \"mae:\", round(mae_cst, 4))\n",
    "        # 打印最佳的 alpha 值和对应的最佳 MAE 值\n",
    "        print(\"best_alpha:\", round(best_alpha_cst, 2), \"best_mape:\", round(best_mape_cst, 4))\n",
    "        # print(\"best_alpha:\", round(best_alpha_cst, 2), \"best_mae:\",round(best_mae_cst, 4))\n",
    "        # 返回最佳的 alpha 值和最佳的 MAE 值\n",
    "        return best_alpha_cst, best_mape_cst\n",
    "\n",
    "    # SBA参数迭代\n",
    "    def sba_optimizer(data):\n",
    "        best_mape_sba = 999\n",
    "        # best_mae_sba = 999\n",
    "        alphas = np.arange(0, 1, 0.1)\n",
    "        i = 0\n",
    "        for alpha in alphas:\n",
    "            i = i + 1\n",
    "            result_sba = croston_SBA(data, alpha)\n",
    "            forecast_sba = result_sba['Forecast']\n",
    "\n",
    "            mape_sba = np.mean(np.abs((forecast_sba[:-1].values - data.values) / data.values))\n",
    "            # mae_sba = mean_absolute_error(data.values, forecast_sba[:-1].values)\n",
    "\n",
    "            if mape_sba < best_mape_sba:\n",
    "                best_alpha_sba, best_mape_sba = alpha, mape_sba\n",
    "            print('sba第', i, '次迭代')\n",
    "            print(\"alpha:\", round(alpha, 2), \"mape:\", round(mape_sba, 4))\n",
    "            # print(\"alpha:\", round(alpha, 2), \"mae:\", round(mae_sba, 4))\n",
    "        # 打印最佳的 alpha 值和对应的最佳 MAE 值\n",
    "        print(\"best_alpha:\", round(best_alpha_sba, 2), \"best_mape:\", round(best_mape_sba, 4))\n",
    "        # print(\"best_alpha:\", round(best_alpha_sba, 2), \"best_mae:\",round(best_mae_sba, 4))\n",
    "        # 返回最佳的 alpha 值和最佳的 MAE 值\n",
    "        return best_alpha_sba, best_mape_sba\n",
    "\n",
    "    # TSB参数优化\n",
    "    def tsb_optimizer(data):\n",
    "        best_mape_tsb = 999\n",
    "        # best_mae_tsb = 999\n",
    "        alphas = np.arange(0, 1, 0.1)\n",
    "        betas = np.arange(0, 1, 0.1)\n",
    "\n",
    "        i = 0\n",
    "        for beta in betas:\n",
    "            for alpha in alphas:\n",
    "                i = i + 1\n",
    "                result_tsb = croston_tsb(data, alpha, beta)\n",
    "                forecast_tsb = result_tsb['Forecast']\n",
    "\n",
    "                mape_tsb = np.mean(np.abs((forecast_tsb[:-1].values - data.values) / data.values))\n",
    "                # mae_tsb = mean_absolute_error(data.values, forecast_tsb[:-1].values)\n",
    "                if mape_tsb < best_mape_tsb:\n",
    "                    best_alpha_tsb, best_beta_tsb, best_mape_tsb = alpha, beta, mape_tsb\n",
    "                print('tsb第', i, '次迭代')\n",
    "                print(\"alpha:\", round(alpha, 2), \"beta:\", round(beta, 2), \"mape:\", round(mape_tsb, 4))\n",
    "            # 打印最佳的 alpha 值和对应的最佳 MAE 值\n",
    "            print(\"best_alpha:\", round(best_alpha_tsb, 2), \"best_beta:\", round(best_beta_tsb, 2), \"best_mape:\",\n",
    "                  round(best_mape_tsb, 4))\n",
    "            # print(\"best_alpha:\", round(best_alpha_tsb, 2), \"best_beta:\", round(best_beta_tsb, 2), \"best_mae:\",\n",
    "            #       round(best_mae_tsb, 4))\n",
    "            # 返回最佳的 alpha 值和最佳的 MAE 值\n",
    "        return best_alpha_tsb, best_beta_tsb, best_mape_tsb\n",
    "\n",
    "    # 调用\n",
    "    df = preprocess_try()\n",
    "    # df = preprocess()\n",
    "    data = df.iloc[:, n]\n",
    "\n",
    "    # 读取列索引名\n",
    "    col = df.columns\n",
    "\n",
    "    # 将数据集分为训练集和测试集 65/68= 0.955 预测后面四个\n",
    "    train_size = int(0.8 * len(data))\n",
    "    train_data = data.iloc[:train_size]\n",
    "    test_data = data.iloc[train_size:]\n",
    "\n",
    "    # 保存最优值\n",
    "    best_alpha_cst, best_mape_cst = cst_optimizer(data)\n",
    "    best_alpha_sba, best_mape_sba = sba_optimizer(data)\n",
    "    best_alpha_tsb, best_beta_tsb, best_mape_tsb = tsb_optimizer(data)\n",
    "    print('CR输入', best_alpha_cst, best_mape_cst)\n",
    "    print('SBA输入', best_alpha_sba, best_mape_sba)\n",
    "    print('TSB输入', best_alpha_tsb, best_beta_tsb, best_mape_tsb)\n",
    "    # 获取预测值\n",
    "    result_cr = croston(data, best_alpha_cst)\n",
    "    result_sba = croston_SBA(data, best_alpha_sba)\n",
    "    result_tsb = croston_tsb(data, best_alpha_tsb, best_beta_tsb)\n",
    "    print('CR:', result_cr)\n",
    "    print('SBA:', result_sba)\n",
    "    print('TSB:', result_tsb)\n",
    "    forecast_cst = result_cr['Forecast']\n",
    "    forecast_sba = result_sba['Forecast']\n",
    "    forecast_tsb = result_tsb['Forecast']\n",
    "\n",
    "    # 画图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['time'], data, label='实际数据', color='black')\n",
    "    plt.plot(df['time'], forecast_cst[:-1], label='预测数据—CR', color='g')\n",
    "    plt.plot(df['time'], forecast_tsb[:-1], label='预测数据—TSB', color='r')\n",
    "    plt.plot(df['time'], forecast_sba[:-1], label='预测数据—SBA', color='b')\n",
    "    plt.xlabel('时间')\n",
    "    plt.ylabel('需求')\n",
    "    plt.title(\n",
    "        f'物料号：{col[n]}\\n CST MAPE:{round(best_mape_cst, 2)}\\n SBA MAPE:{round(best_mape_sba, 2)} \\n TSB MAPE:{round(best_mape_tsb, 2)}')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cst')\n",
    "    # plt.show()\n",
    "    return best_mape_cst, best_mape_sba, best_mape_tsb,col\n",
    "# crosotn及其变体\n",
    "def CRO_run():\n",
    "    # 创建一个空列表，用于存储 MAPE 值\n",
    "    cro_mape_values = []\n",
    "    for n in data_CRO:\n",
    "        # 运行 SMAforecast_try() 并计算 MAPE\n",
    "        best_mape_cst, best_mape_sba, best_mape_tsb, col = Croston_try(n)\n",
    "        # 取出对应的物料号\n",
    "        material_number = col[n]\n",
    "        print(\"这是\", material_number)\n",
    "        print( best_mape_cst, best_mape_sba, best_mape_tsb)\n",
    "        # 将 MAPE 值作为元组添加到列表中\n",
    "        cro_mape_values.append((material_number, best_mape_cst, best_mape_sba, best_mape_tsb))\n",
    "    # 将 MAPE 值写入文件\n",
    "    with open(\"CRO不平稳=.txt\", \"w\") as f:\n",
    "        # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "        for material_number, best_mape_cst, best_mape_sba, best_mape_tsb in cro_mape_values:\n",
    "            f.write(\n",
    "                f\"物料号：{material_number}, MAPE CRO: {round(best_mape_cst, 2)}, MAPE SBA: {round(best_mape_sba, 2)},MAPE TSB:{round(best_mape_tsb, 2)}\\n\")\n",
    "    csv_filename = \"CRO不平稳杭宁.csv\"\n",
    "    with open(csv_filename, \"a\", newline='') as f:  # 使用追加模式，并且指定 newline='' 来避免空行\n",
    "        writer = csv.writer(f)\n",
    "        # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "        for material_number, best_mape_cst, best_mape_sba, best_mape_tsb in cro_mape_values:\n",
    "            writer.writerow(\n",
    "                [material_number, round(best_mape_cst, 2), round(best_mape_sba, 2), round(best_mape_tsb, 2)])\n",
    "    plt.show()\n",
    "    return\n",
    "if __name__ == '__main__':\n",
    "    # 物料取值\n",
    "    data_CRO = range(1,2)\n",
    "    # 运行\n",
    "    CRO_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ca098f-5aa2-4585-a052-704c6ad28553",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: ANSI",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m data_FST \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# 运行\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[43mFST_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 112\u001b[0m, in \u001b[0;36mFST_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m fst_mape_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m data_FST:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# 运行 SMAforecast_try() 并计算 MAPE\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     min_mape, col \u001b[38;5;241m=\u001b[39m \u001b[43mforest_try\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# 取出对应的物料号\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     material_number \u001b[38;5;241m=\u001b[39m col[n]\n",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m, in \u001b[0;36mforest_try\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforest_try\u001b[39m(n):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# 读取数据\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_try\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     col \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# %%\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 195\u001b[0m, in \u001b[0;36mpreprocess_try\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_try\u001b[39m():\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 读取CSV文件\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m不稳定杭宁.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANSI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新不稳定需求_季度_ADI=1_hz.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新需求平稳_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv(\"新不稳定需求_季度_ADI=1_nb.csv\", encoding=\"ANSI\")\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# 定义一个函数，用于将负数转换为正数\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_positive\u001b[39m(x):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:723\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    720\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# validate encoding and errors\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(errors, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n",
      "\u001b[0;31mLookupError\u001b[0m: unknown encoding: ANSI"
     ]
    }
   ],
   "source": [
    "#随机森林预测\n",
    "def forest_try(n):\n",
    "    # 读取数据\n",
    "    df = preprocess_try()\n",
    "\n",
    "    col = df.columns\n",
    "\n",
    "    # %%\n",
    "    df = df.iloc[:, [0, n]]\n",
    "    df.columns = ['time', 'values']\n",
    "\n",
    "    #\n",
    "    # 特征工程\n",
    "    # 解析季度数据为年份和季度\n",
    "    df['Year'] = df['time'].str.split(' 第').str[0].astype(int)\n",
    "    df['Quarter'] = df['time'].str.split('第').str[1].str.replace('季度', '').astype(int)\n",
    "\n",
    "    # 划分数据集\n",
    "    train_size = int(0.82 * len(df))\n",
    "    train_data = df.iloc[:train_size]\n",
    "    test_data = df.iloc[train_size:]\n",
    "\n",
    "    # 准备特征和目标变量\n",
    "    X_train = train_data[['Year', 'Quarter']]\n",
    "    y_train = train_data['values']\n",
    "    X_test = test_data[['Year', 'Quarter']]\n",
    "    y_test = test_data['values']\n",
    "\n",
    "    # print(X_train,y_train)\n",
    "    # 创建和训练随机森林模型\n",
    "\n",
    "    # 设置要尝试的参数值列表\n",
    "    # n_estimators_list = [5,20,40,60,80, 100, 120]\n",
    "    # max_depth_list = [10,20, 40, 60,80,100]\n",
    "\n",
    "    n_estimators_list = np.arange(5,100,5)\n",
    "    max_depth_list = np.arange(5,100,5)\n",
    "\n",
    "    # 初始化最小 MAPE 和对应的参数组合\n",
    "    min_mape = float('inf')\n",
    "    best_n_estimators = None\n",
    "    best_max_depth = None\n",
    "    best_predicted = None\n",
    "    # 循环尝试不同的参数组合\n",
    "    for n_estimators in n_estimators_list:\n",
    "        for max_depth in max_depth_list:\n",
    "            # 创建和训练随机森林模型\n",
    "            rf_model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                             random_state=0,max_depth=max_depth)\n",
    "            rf_model.fit(X_train, y_train)\n",
    "            # 在测试集上评估模型性能\n",
    "            y_pred = rf_model.predict(X_test)\n",
    "            mape = np.mean(np.abs((y_pred - y_test) / y_test))\n",
    "\n",
    "            # 更新最小 MAPE 和对应的参数组合\n",
    "            if mape < min_mape:\n",
    "                min_mape = mape\n",
    "                best_n_estimators = n_estimators\n",
    "                best_max_depth = max_depth\n",
    "                best_predicted = y_pred\n",
    "                # 打印参数组合和性能\n",
    "            print(f\"n_estimators={n_estimators}, max_depth={max_depth}, MAPE={mape},Best_predicted={y_pred}\")\n",
    "\n",
    "    # 打印最佳参数组合和最小的 MAPE\n",
    "    print(f\"Best parameters: n_estimators={best_n_estimators}, max_depth={best_max_depth}, Min MAPE={min_mape},Best_predicted={best_predicted}\")\n",
    "\n",
    "    # # 预测未来3个月的销售额\n",
    "    # next_months = pd.date_range(start=data.index[-1], periods=3, freq='M')\n",
    "    # next_months_data = pd.DataFrame({'Month': next_months.month}, index=next_months)\n",
    "    # predicted_sales = rf_model.predict(next_months_data[['Month']])\n",
    "\n",
    "    # 打印预测结果\n",
    "    # print(\"预测未来3个月的需求：\")\n",
    "    # print(predicted_sales)\n",
    "\n",
    "    # 计算模型性能\n",
    "    # y_pred = rf_model.predict(X_test)\n",
    "    # mape = np.mean(np.abs((y_pred - y_test) / y_test))\n",
    "    print(\"MAPE：\", min_mape)\n",
    "    # mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # 将预测结果与真实数据绘制成图表\n",
    "    # 在绘图前指定字体\n",
    "    # plt.rcParams[\"font.family\"] = \"SimHei\"  # 使用宋体字体\n",
    "    # plt.rcParams[\"font.size\"] = 12  # 设置字体大小\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(df['time'], df['values'], label='真实数量', color='black')\n",
    "    last_five_time = df['time'].tail(4)\n",
    "    ax.plot(last_five_time, best_predicted, label='预测数量', color='r')\n",
    "\n",
    "    # ax.plot(next_months, predicted_sales, label='未来3个月预测', color='g')\n",
    "\n",
    "    # 其他绘图设置\n",
    "    plt.xlabel('日期')\n",
    "    plt.ylabel('数量')\n",
    "    plt.title(f'物料号：{col[n]} \\n MAPE:{round(min_mape, 2)}')\n",
    "\n",
    "    plt.legend()\n",
    "    # plt.grid(True)\n",
    "    plt.xticks(rotation=45)  # 如果需要旋转刻度标签\n",
    "    plt.tight_layout()  # 确保标签不重叠\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('FST')\n",
    "    plt.show()\n",
    "    return min_mape,col\n",
    "def FST_run():\n",
    "    # 创建一个空列表，用于存储 MAPE 值\n",
    "    fst_mape_values = []\n",
    "    for n in data_FST:\n",
    "        # 运行 SMAforecast_try() 并计算 MAPE\n",
    "        min_mape, col = forest_try(n)\n",
    "        # 取出对应的物料号\n",
    "        material_number = col[n]\n",
    "        print(\"物料号\", material_number)\n",
    "        print(min_mape)\n",
    "        # 将 MAPE 值作为元组添加到列表中\n",
    "        fst_mape_values.append((material_number, min_mape))\n",
    "    # 将 MAPE 值写入文件\n",
    "    with open(\"FST不平稳杭宁.txt\", \"w\") as f:\n",
    "        # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "        for material_number, min_mape in fst_mape_values:\n",
    "            f.write(\n",
    "                f\"物料号：{material_number}, MAPE FSR: {round(min_mape, 2)}\\n\")\n",
    "    csv_filename = \"FST不平稳杭宁.csv\"\n",
    "    with open(csv_filename, \"a\", newline='') as f:  # 使用追加模式，并且指定 newline='' 来避免空行\n",
    "        writer = csv.writer(f)\n",
    "        # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "        for material_number,min_mape in fst_mape_values:\n",
    "            writer.writerow(\n",
    "                [material_number, round(min_mape, 2)])\n",
    "\n",
    "    plt.show()\n",
    "    return\n",
    "if __name__ == '__main__':\n",
    "    # 物料取值\n",
    "    data_FST = range(1,2)\n",
    "    # 运行\n",
    "    FST_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f94703-469d-4150-b1aa-cf5b26895aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA预测\n",
    "#基础 ARIMA 模型\n",
    "def ARIMAforecast_try():\n",
    "    # 重采样\n",
    "    def Resampling(data):\n",
    "        df = data\n",
    "        # df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index(\"time\", inplace=True)\n",
    "        train_data = df['2018 第1季度':'2022 第1季度']\n",
    "        test_data = df['2022 第2季度':'2023 第2季度']\n",
    "        # 以月为时间间隔取均值,重采样\n",
    "        # train_data = train_data.resample('M').mean()\n",
    "        # test_data = test.resample('M').mean()\n",
    "        # print(train_data, test_data)\n",
    "        return train_data, test_data\n",
    "\n",
    "    #### Step 3  差分转平稳\n",
    "    def stationarity(timeseries):  # 平稳性处理（timeseries 时间序列）\n",
    "        ## 差分法,保存成新的列\n",
    "        diff1 = timeseries.diff(1).dropna()  # 1阶差分 dropna() 删除缺失值\n",
    "        diff2 = diff1.diff(1).dropna()  # 在一阶差分基础上再做一次一阶差分，即二阶查分\n",
    "        ## 画图\n",
    "        # diff1.plot(color='red', title='diff 1', figsize=(10, 4))\n",
    "        # diff2.plot(color='black', title='diff 2', figsize=(10, 4))\n",
    "        ## 平滑法\n",
    "        rollmean = timeseries.rolling(window=4, center=False).mean().dropna()  ## 滚动平均\n",
    "        rollstd = timeseries.rolling(window=4, center=False).std().dropna()  ## 滚动标准差\n",
    "        ## 画图\n",
    "        # rollmean.plot(color='yellow', title='Rolling Mean', figsize=(10, 4))\n",
    "        # rollstd.plot(color='blue', title='Rolling Std', figsize=(10, 4))\n",
    "        return diff1, diff2, rollmean, rollstd\n",
    "\n",
    "    # 平稳性检验\n",
    "    #### Step 4  平稳性检验\n",
    "    def ADF_test(timeseries):  ## 用于检测序列是否平稳\n",
    "        column_name = timeseries.columns[0]\n",
    "        # print(column_name)\n",
    "        # print(timeseries)\n",
    "        x = np.array(timeseries[column_name])\n",
    "        # x = np.array(timeseries['values'])\n",
    "        adftest = adfuller(x, autolag='AIC')\n",
    "        print(adftest)\n",
    "        if adftest[0] < adftest[4][\"1%\"] and adftest[1] < 10 ** (-8):\n",
    "            # 对比Adf结果和10%的时的假设检验 以及 P-value是否非常接近0(越小越好)\n",
    "            print(\"序列平稳\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"非平稳序列\")\n",
    "            return False\n",
    "\n",
    "    # 随机性检验（白噪声检验）\n",
    "    def random_test(timeseries):\n",
    "        p_value = acorr_ljungbox(timeseries, lags=1)  # p_value 返回二维数组，第二维为P值\n",
    "        print(p_value)\n",
    "        if p_value['lb_pvalue'].iloc[0] < 0.05:\n",
    "            print(\"非随机性序列\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"随机性序列,即白噪声序列\")\n",
    "            return False\n",
    "\n",
    "    # 利用ACF和PACF判断模型阶数\n",
    "    def determinate_order_acf(timeseries):\n",
    "        plot_acf(timeseries, lags=16)  # 延迟数\n",
    "        plot_pacf(timeseries, lags=7)\n",
    "        plt.show()\n",
    "\n",
    "    # ARMA模型\n",
    "    def ARMA_model(train_data, order):\n",
    "        arma_model = ARIMA(train_data, order=order)\n",
    "        arma = arma_model.fit()\n",
    "        # print(result.summary()) #给出一份模型报告\n",
    "        # 样本内预测\n",
    "        in_sample_pred = arma.predict()\n",
    "        # 样本外预测\n",
    "        out_sample_pred = arma.predict(start=len(train_data) - 1, end=len(train_data) + 11, dynamic=True)\n",
    "        # in_sample_pred.plot()\n",
    "        # train_data.plot()\n",
    "        return arma, in_sample_pred, out_sample_pred\n",
    "\n",
    "    # ARIMA模型\n",
    "    def ARIMA_model(train_data, order):\n",
    "        arima_model = ARIMA(train_data, order=order)\n",
    "        arima = arima_model.fit()\n",
    "        # print(result.summary()) # 给出一份模型报告\n",
    "        # 样本内预测\n",
    "        in_sample_pred = arima.predict()\n",
    "        # 样本外预测\n",
    "        out_sample_pred = arima.predict(start=len(train_data) - 1, end=len(train_data) + 4, dynamic=True)\n",
    "        # print(out_sample_pred)\n",
    "        # print(in_sample_pred)\n",
    "        return arima, in_sample_pred, out_sample_pred\n",
    "\n",
    "    # 模型评估\n",
    "    def evaluate_model(model, train_data, predict_data):\n",
    "        # （1）利用QQ图检验残差是否满足正态分布\n",
    "        resid = model.resid  # 求解模型残差\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        qqplot(resid, line='q', fit=True)\n",
    "        plt.xlabel(\"理论分位数\")\n",
    "        plt.ylabel(\"样本分位数\")\n",
    "        plt.legend([\"残差\"], loc=\"upper left\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        # （2）利用D-W检验,检验残差的自相关性\n",
    "        print('D-W检验值为{}'.format(durbin_watson(resid.values)))\n",
    "        # （3）利用预测值和真实值的误差检测，这里用的是标准差\n",
    "        print('标准差为{}'.format(mean_squared_error(train_data, predict_data, sample_weight=None,\n",
    "                                                 multioutput='uniform_average')))  # 标准差（均方差）\n",
    "\n",
    "    def draw_picture(row_train_data, out_sample_pred_arima, test_data):\n",
    "        # print(out_sample_pred)\n",
    "        # 样本外预测传入 test_data,out_sample_pred\n",
    "        # 由于预测都是由差分后的平稳序列得出,因此需要对差分后的数据进行还原\n",
    "        # 还原后绘制同一起点的曲线\n",
    "        # 将差分后的序列还原,re_out_sample_pred为还原之后\n",
    "        # re_out_sample_pred_arma = pd.Series(np.array(row_train_data)[-2][0], index=[row_train_data.index[-2]])._append(out_sample_pred_arma[1:]).cumsum()\n",
    "        re_out_sample_pred_arima = pd.Series(np.array(row_train_data)[-2][0], index=[row_train_data.index[-2]])._append(\n",
    "            out_sample_pred_arima[1:]).cumsum()\n",
    "\n",
    "        # # 横坐标\n",
    "        # x = []\n",
    "        # start_value = 45  # 设置起始值为44\n",
    "        # for i in range(24):\n",
    "        #     x.append(start_value + i)\n",
    "        # x = np.array(x)\n",
    "        # x_1 = []\n",
    "        # for i in range(44):\n",
    "        #     x_1.append(i + 1)\n",
    "        # x_1 = np.array(x_1)\n",
    "        df_all = preprocess_try()\n",
    "        x = df_all.iloc[:, [0]]\n",
    "        # print(x)\n",
    "        # 使用between方法来切片时间列\n",
    "        # x_train_time = x[x['time'].between('2018 第1季度', '2022 第2季度')]\n",
    "        # x_test_time = x[x['time'].between('2022 第3季度', '2023 第2季度')]\n",
    "\n",
    "        # 纵坐标\n",
    "        y1 = np.array(row_train_data)\n",
    "        y2 = np.array(test_data)\n",
    "        # y3 = np.array(re_out_sample_pred_arma[1:])\n",
    "\n",
    "        y4 = np.array(re_out_sample_pred_arima[1:])\n",
    "        print(y4)\n",
    "        mape = np.mean(np.abs((y4 - y2) / y2))\n",
    "\n",
    "        # 画图\n",
    "        plt.plot(train_data.index, y1, label='Training Data')\n",
    "        plt.plot(test_data.index, y2, color='b', label='Test Data')\n",
    "        # plt.plot(x_test_time, y3, color='r', label='ARMA Prediction')\n",
    "        print(test_data.index)\n",
    "        plt.plot(test_data.index, y4, color='r', label='ARIMA Prediction')\n",
    "        plt.title(f\"{df_all.columns[n]},MAPE_ARIMA:{round(mape, 2)}\")\n",
    "        # 添加图例\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=90)\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "        return mape\n",
    "\n",
    "    # 参数\n",
    "\n",
    "    # n = 6                        # 读取的列索引序号\n",
    "\n",
    "    # 数据预处理\n",
    "    df = preprocess_try()\n",
    "    df = df.iloc[:, [0, n]]  # 选择第0列和第n列，这将保留时间列和第n列\n",
    "\n",
    "    # # 重采样,划分训练测试\n",
    "    train_data, test_data = Resampling(df)\n",
    "    train_data = train_data.astype(float)\n",
    "    row_train_data = train_data  # 保存差分前的序列,为了后面做评估\n",
    "    print(row_train_data)\n",
    "    # 差分转平稳\n",
    "    Smooth_data = stationarity(train_data)\n",
    "    print(Smooth_data)\n",
    "\n",
    "    # for data in zip(Smooth_data, range(4)):             # range(4) 用于判断哪种方法 满足平稳性和白噪声 diff1 diff2 rollmean,rollstd\n",
    "    #     # print(data[0])\n",
    "    #     if ADF_test(data[0]) and random_test(data[0]):  # 平稳性和白噪声检测\n",
    "    #         train_data = data[0]                        # 先用差分，再用平滑,分别对应4个序列\n",
    "    #         method = data[1]\n",
    "    #         print(method)                               # 如果是差分做的,那么后面ARIMA模型中要使用这个参数\n",
    "    #         break\n",
    "    # print(train_data)\n",
    "\n",
    "    # determinate_order_acf(train_data)  # ACF定阶\n",
    "\n",
    "    # 参数\n",
    "    # order_arma = (1, 0, 1)  # ARMA      p,q其中d=0\n",
    "    order_arima = (2, 1, 1)  # ARIMA     p,d,q\n",
    "\n",
    "    # print(order_arma.dtypes)\n",
    "    # 调用模型\n",
    "    # arma, in_sample_pred_arma, out_sample_pred_arma = ARMA_model(train_data, order_arma)\n",
    "    arima, in_sample_pred_arima, out_sample_pred_arima = ARIMA_model(train_data, order_arima)\n",
    "\n",
    "    # #\n",
    "    # 模型评价\n",
    "    # evaluate_model(arma, train_data, in_sample_pred_arma)\n",
    "    # evaluate_model(arima, train_data, in_sample_pred_arima)\n",
    "    # #\n",
    "    # # 可视化\n",
    "    draw_picture(row_train_data, out_sample_pred_arima, test_data)\n",
    "\n",
    "#SARIMA模型（考虑季节性）\n",
    "def ARIMA_try(n):\n",
    "    # Load the data\n",
    "    data = preprocess_try()\n",
    "    col = data.columns\n",
    "    data = data.iloc[:, [0, n]]\n",
    "    # A bit of pre-processing to make it nicer\n",
    "    data.set_index(['time'], inplace=True)\n",
    "\n",
    "    #\n",
    "    def convert_to_first_month(quarter_str):\n",
    "        year, quarter = quarter_str.split(' 第')\n",
    "        year = int(year)\n",
    "        quarter = int(quarter[0])\n",
    "\n",
    "        if quarter == 1:\n",
    "            month = 1\n",
    "        elif quarter == 2:\n",
    "            month = 4\n",
    "        elif quarter == 3:\n",
    "            month = 7\n",
    "        elif quarter == 4:\n",
    "            month = 10\n",
    "\n",
    "        return f\"{year}-{month}-1\"\n",
    "\n",
    "    #\n",
    "    # 将data中\"time\"列的值根据convert_to_first_month函数进行转换\n",
    "    data.index = data.index.map(convert_to_first_month)\n",
    "    # 将索引转换为日期时间类型\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    #\n",
    "    # # 显示更改后的数据\n",
    "    print(data)\n",
    "    #\n",
    "    # 划分训练测试\n",
    "    train_data = data['2018-4-1':'2022-4-1']\n",
    "    test_data = data['2022-7-1':'2023-7-1']\n",
    "    train_data = train_data.astype(float)  # 将数据转换为浮点数类型\n",
    "    test_data = test_data.astype(float)  # 将数据转换为浮点数类型\n",
    "    print(test_data)\n",
    "\n",
    "    # 定义 d 和 q 参数，它们的取值范围在 0 到 1 之间\n",
    "    q = range(0, 3)\n",
    "    d = range(0, 2)\n",
    "    # 要定义 p 参数，其取值范围在 0 到 3 之间\n",
    "    p = range(0, 4)\n",
    "\n",
    "    # 生成所有不同的 p、d 和 q 参数的组合。\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "    # 生成所有不同的季节性 p、d 和 q 参数的组合\n",
    "    seasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "    print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "    print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "    print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "    print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "    print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))\n",
    "\n",
    "\n",
    "    AIC = []\n",
    "    SARIMAX_model = []\n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            try:\n",
    "                mod = sm.tsa.statespace.SARIMAX(train_data,\n",
    "                                                order=param,\n",
    "                                                seasonal_order=param_seasonal,\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False)\n",
    "\n",
    "                results = mod.fit()\n",
    "\n",
    "                print('SARIMAX{}x{} - AIC:{}'.format(param, param_seasonal, results.aic), end='\\r')\n",
    "                AIC.append(results.aic)\n",
    "                SARIMAX_model.append([param, param_seasonal])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    print('The smallest AIC is {} for model SARIMAX{}x{}'.format(min(AIC), SARIMAX_model[AIC.index(min(AIC))][0],\n",
    "                                                                 SARIMAX_model[AIC.index(min(AIC))][1]))\n",
    "\n",
    "    # Let's fit this model\n",
    "    mod = sm.tsa.statespace.SARIMAX(train_data,\n",
    "                                    order=SARIMAX_model[AIC.index(min(AIC))][0],\n",
    "                                    seasonal_order=SARIMAX_model[AIC.index(min(AIC))][1],\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False)\n",
    "\n",
    "    results = mod.fit()\n",
    "    # print('zheshi',results.aic)\n",
    "    # results.plot_diagnostics(figsize=(20, 14))\n",
    "    plt.show()\n",
    "\n",
    "    # 样本内预测——无动态 4\n",
    "    pred0 = results.get_prediction(start='2021-7-01', dynamic=False)\n",
    "    pred0_ci = pred0.conf_int()\n",
    "    data_pred0 = data['2021-07-01':'2022-04-01']\n",
    "\n",
    "    # 样本内预测——动态 4\n",
    "    pred1 = results.get_prediction(start='2021-07-01', dynamic=True)\n",
    "    pred1_ci = pred1.conf_int()\n",
    "    data_pred1 = data['2021-07-01':'2022-04-01']\n",
    "    # print('这个hi',data_pred1.index)\n",
    "\n",
    "    # 样本外预测\n",
    "    pred2 = results.get_forecast('2023-04-01')\n",
    "    pred2_ci = pred2.conf_int()\n",
    "    print(pred2.predicted_mean['2022-07-01':'2023-04-01'])\n",
    "    # 生成要添加的日期范围\n",
    "    # new_dates = pd.date_range(start='2023-10-01', end='2024-04-01', freq='3M')\n",
    "    # 将新日期索引附加到现有索引上\n",
    "    # data_more = data._append(pd.DataFrame(index=new_dates))\n",
    "    # 重新排序索引\n",
    "    # data_more = data_more.sort_index()\n",
    "    # print(data_more)\n",
    "    data_pred2 = data['2022-07-01':'2023-04-01']\n",
    "    # print(data_pred2.index)\n",
    "    # print( pred2.predicted_mean)\n",
    "\n",
    "    prediction = pred2.predicted_mean['2022-07-01':'2023-04-01'].values\n",
    "    # print(prediction)\n",
    "    # flatten nested list\n",
    "    truth = list(itertools.chain.from_iterable(test_data.values))\n",
    "    MAPE = np.mean(np.abs((truth - prediction) / truth))\n",
    "    # MAE = mean_absolute_error(truth, prediction)\n",
    "    print('MAPE:', MAPE)\n",
    "    # print('MAE :', MAE)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data.index, data, label='原始数据', color='black')\n",
    "    # plt.plot(data_pred0.index, pred0.predicted_mean, label='无动态样本内预测', color='b')\n",
    "    # plt.plot(data_pred1.index, pred1.predicted_mean, label='动态样本内预测', color='g')\n",
    "    plt.plot(data_pred2.index, pred2.predicted_mean, label='样本外预测', color='r')\n",
    "    plt.ylabel('数量')\n",
    "    plt.xlabel('日期')\n",
    "    plt.xticks(rotation=45)  # 如果需要旋转刻度标签\n",
    "    plt.title(f'物料号：{col[n]} \\n MAPE:{round(MAPE, 2)}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('SARIMA')\n",
    "    plt.show()\n",
    "    return MAPE,col\n",
    "def ARI_run():\n",
    "    # 创建一个空列表，用于存储 MAPE 值\n",
    "    ari_mape_values = []\n",
    "    for n in data_ARI:\n",
    "        # 运行 SMAforecast_try() 并计算 MAPE\n",
    "        MAPE, col = ARIMA_try(n)\n",
    "        # 取出对应的物料号\n",
    "        material_number = col[n]\n",
    "        print(\"物料号\", material_number)\n",
    "        print(MAPE)\n",
    "        # 将 MAPE 值作为元组添加到列表中\n",
    "        ari_mape_values.append((material_number, MAPE))\n",
    "    # 将 MAPE 值写入文件\n",
    "    with open(\"ARI平稳杭宁_5.txt\", \"w\") as f:\n",
    "        # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "        for material_number, MAPE in ari_mape_values:\n",
    "            f.write(\n",
    "                f\"物料号：{material_number}, MAPE ARI: {round(MAPE, 2)}\\n\")\n",
    "    csv_filename = \"ARI平稳杭宁_5.csv\"\n",
    "    with open(csv_filename, \"a\", newline='') as f:  # 使用追加模式，并且指定 newline='' 来避免空行\n",
    "        writer = csv.writer(f)\n",
    "        # 循环遍历每个 n 对应的 MAPE 值，并将其写入文件\n",
    "        for material_number,MAPE in ari_mape_values:\n",
    "            writer.writerow(\n",
    "                [material_number, round(MAPE, 2)])\n",
    "\n",
    "    return\n",
    "if __name__ == '__main__':\n",
    "    # 物料取值\n",
    "    data_ARI = range(1,2)\n",
    "    # 运行\n",
    "    ARI_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772908d2-b866-4eae-8d15-2c838443f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91662c6-125d-4ae9-a840-316d890d6a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/bulingzheng/Library/Python/3.9/lib/python/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/bulingzheng/Library/Python/3.9/lib/python/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m  \u001b[38;5;66;03m# 数据可视化库\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydot\u001b[39;00m\n\u001b[1;32m     19\u001b[0m matplotlib\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTkAgg\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 或者使用 'Qt5Agg'\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective, dask\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Booster,\n\u001b[1;32m     10\u001b[0m     DataIter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     build_info,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/tracker.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, make_jcargs\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:269\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m        return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:222\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[1;32m    221\u001b[0m         libname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) could not be loaded.\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124mLikely causes:\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124m  * OpenMP runtime is not installed\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;124m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[38;5;124mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m     _register_log_callback(lib)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(ver: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mXGBoostError\u001b[0m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/bulingzheng/Library/Python/3.9/lib/python/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/bulingzheng/Library/Python/3.9/lib/python/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "#XGBoost预测\n",
    "import forecast\n",
    "import warnings, pandas as pd, numpy as np, math  # 告警库 数据处理库  科学计算库\n",
    "import matplotlib #引入画图数据库\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.fftpack as fftpack\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "import matplotlib.dates as mdates\n",
    "# 导入第三方库\n",
    "import numpy.random as rd  # 导入随机数生成方法\n",
    "import matplotlib.pyplot as plt  # 数据可视化库\n",
    "import matplotlib\n",
    "import xgboost as xgb\n",
    "import pydot\n",
    "matplotlib.use('TkAgg')  # 或者使用 'Qt5Agg'\n",
    "import seaborn as sns  # 高级数据可视化库\n",
    "from sklearn.model_selection import train_test_split  # 数据集拆分工具\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import warnings, pandas as pd, numpy as np, math  # 告警库 数据处理库  科学计算库\n",
    "from keras.models import Sequential  # 导入序贯模型\n",
    "from keras.layers import Dense  # 导入全连接层\n",
    "from keras.layers import LSTM  # 导入LSTM层\n",
    "from keras.utils import plot_model  # 导入 模型绘图工具\n",
    "import keras.layers as layers  # 导入 层 工具包\n",
    "import keras.backend as K  # 导入 后端 工具包\n",
    "import tensorflow as tf  # 导入 TensorFlow 库\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, mean_absolute_error, r2_score  # 模型评估方法\n",
    "\n",
    "warnings.filterwarnings('ignore')  # 忽略告警\n",
    "\n",
    "# 定义灰狼优化算法实现函数\n",
    "def sanitized_gwo(X_train, X_test, y_train, y_test, SearchAgents_no, T, dim, lb, ub):\n",
    "    Alpha_position = [0, 0]  # 初始化Alpha灰狼的位置\n",
    "    Beta_position = [0, 0]  # 初始化Beta灰狼的位置\n",
    "    Delta_position = [0, 0]  # 初始化Delta灰狼的位置\n",
    "\n",
    "    Alpha_score = float(\"inf\")  # 初始化Alpha灰狼目标函数的值\n",
    "    Beta_score = float(\"inf\")  # 初始化Beta灰狼目标函数的值\n",
    "    Delta_score = float(\"inf\")  # 初始化Delta灰狼目标函数的值\n",
    "\n",
    "    Positions = np.dot(rd.rand(SearchAgents_no, dim), (ub - lb)) + lb  # 初始化第一个搜索位置\n",
    "\n",
    "    Convergence_curve = np.zeros((1, T))  # 初始化收敛曲线\n",
    "\n",
    "    iterations = []  # 定义迭代次数列表\n",
    "    accuracy = []  # 定义准确率列表\n",
    "\n",
    "    # 迭代求解\n",
    "    t = 0\n",
    "    while t < T:  # 循环\n",
    "        # 迭代每只灰狼位置\n",
    "        for i in range(0, (Positions.\n",
    "                shape[0])):\n",
    "            # 如果搜索位置超出了搜索空间，则需要返回到搜索空间\n",
    "            for j in range(0, (Positions.shape[1])):\n",
    "                Flag4ub = Positions[i, j] > ub  # 大于最大值\n",
    "                Flag4lb = Positions[i, j] < lb  # 小于最小值\n",
    "                # 如果灰狼的位置在最大值和最小值之间，则不需要调整位置，如果超过最大值，则返回最大值边界；如果低于最小值，则返回最小值边界\n",
    "                if Flag4ub:  # 判断\n",
    "                    Positions[i, j] = ub  # 赋值\n",
    "                if Flag4lb:  # 判断\n",
    "                    Positions[i, j] = lb  # 赋值\n",
    "\n",
    "            # 建立XGB模型并训练\n",
    "            # XGB预测\n",
    "            model = xgb.XGBRegressor(max_depth=int(abs(Positions[i][0])), learning_rate=int(abs(Positions[i][1]))*0.01, n_estimators=int(abs(Positions[i][2]))*100, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = round(r2_score(y_test, y_pred), 4)  # 计算R2\n",
    "\n",
    "\n",
    "            # 使错误率降到最低\n",
    "            fitness_value = 1 - score  # 错误率 赋值 适应度函数值\n",
    "            if fitness_value < Alpha_score:  # 如果目标函数值小于Alpha灰狼的目标函数值\n",
    "                Alpha_score = fitness_value  # 然后将Alpha灰狼的目标函数值更新为最优目标函数值\n",
    "                Alpha_position = Positions[i]  # 同时更新Alpha灰狼的位置到最佳位置\n",
    "            if fitness_value > Alpha_score and fitness_value < Beta_score:  # 如果目标函数值大于Alpha灰狼的目标函数值并且小于Beta灰狼的目标函数值\n",
    "                Beta_score = fitness_value  # 然后将Beta灰狼的目标函数值更新为最优目标函数值\n",
    "                Beta_position = Positions[i]  # 同时更新Beta灰狼的位置到最佳位置\n",
    "            # 如果目标函数值大于Alpha灰狼的目标函数值并且大于Beta灰狼的目标函数值并且小于Delta灰狼的目标函数值\n",
    "            if fitness_value > Alpha_score and fitness_value > Beta_score and fitness_value < Delta_score:\n",
    "                Delta_score = fitness_value  # 然后将Delta灰狼的目标函数值更新为最优目标函数值\n",
    "                Delta_position = Positions[i]  # 同时更新Delta灰狼的位置到最佳位置\n",
    "\n",
    "        a = 2 - t * (2 / T)  # 收敛因子从2线性递减到0\n",
    "\n",
    "        # 循环更新灰狼个体的位置\n",
    "        for i in range(0, (Positions.shape[0])):\n",
    "            # 遍历每个维度\n",
    "            for j in range(0, (Positions.shape[1])):\n",
    "                # 包围猎物，更新位置\n",
    "                r1 = rd.random(1)  # 生成0~1之间的随机数\n",
    "                r2 = rd.random(1)  # 生成0~1之间的随机数\n",
    "                A1 = 2 * a * r1 - a  # 计算系数向量A\n",
    "                C1 = 0.5 + (0.5 * math.exp(-j / 500)) + (1.4 * (math.sin(j) / 30))  # 通过时变加速度常数 计算系数向量C\n",
    "\n",
    "                # alpha灰狼位置更新\n",
    "                D_alpha = abs(C1 * Alpha_position[j] - Positions[i, j])  # alpha灰狼与其它个体的距离\n",
    "                X1 = Alpha_position[j] - A1 * D_alpha  # alpha灰狼当前的位置\n",
    "\n",
    "                r1 = rd.random(1)  # 生成0~1之间的随机数\n",
    "                r2 = rd.random(1)  # 生成0~1之间的随机数\n",
    "\n",
    "                A2 = 2 * a * r1 - a  # 计算系数向量A\n",
    "                C2 = 1 + (1.4 * (1 - math.exp(-j / 500))) + (1.4 * (math.sin(j) / 30))  # 基于差分均值的摄动时变参数 计算系数向量C\n",
    "\n",
    "                # Beta灰狼位置更新\n",
    "                D_beta = abs(C2 * Beta_position[j] - Positions[i, j])  # Beta灰狼与其它个体的距离\n",
    "                X2 = Beta_position[j] - A2 * D_beta  # Beta灰狼当前的位置\n",
    "\n",
    "                r1 = rd.random(1)  # 生成0~1之间的随机数\n",
    "                r2 = rd.random(1)  # 生成0~1之间的随机数\n",
    "\n",
    "                A3 = 2 * a * r1 - a  # 计算系数向量A\n",
    "                C3 = (1 / (1 + math.exp(-0.0001 * j / T))) + (\n",
    "                        (0.5 - 2.5) * ((j / T) ** 2))  # 基于sigmoid函数的加速度系数 计算系数向量C\n",
    "\n",
    "                # Delta灰狼位置更新\n",
    "                D_delta = abs(C3 * Delta_position[j] - Positions[i, j])  # Delta灰狼与其它个体的距离\n",
    "                X3 = Delta_position[j] - A3 * D_delta  # Delta灰狼当前的位置\n",
    "\n",
    "                # 位置更新\n",
    "                Positions[i, j] = (X1 + X2 + X3) / 3\n",
    "\n",
    "        t = t + 1\n",
    "        iterations.append(t)  # 迭代次数存入列表中\n",
    "        accuracy.append(abs((100 - Alpha_score) / 100))  # 计算准确率\n",
    "        print('----------------迭代次数----------------' + str(t))\n",
    "\n",
    "    best_depth = Alpha_position[0]\n",
    "    best_lr = Alpha_position[1]  # 最优位置  即最优参数值\n",
    "    best_estimators = Alpha_position[2]  # 最优位置  即最优参数值\n",
    "\n",
    "    return best_depth, best_lr, best_estimators, iterations, accuracy  # 返回数据\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    df= Forecast.preprocess_try()\n",
    "    col = df.columns\n",
    "    n=16\n",
    "    # n=7\n",
    "    # %%\n",
    "    df = df.iloc[:, [0, n]]\n",
    "    df.columns = ['time', 'values']\n",
    "\n",
    "    #\n",
    "    # 特征工程\n",
    "    # 解析季度数据为年份和季度\n",
    "    df['Year'] = df['time'].str.split(' 第').str[0].astype(int)\n",
    "    df['Quarter'] = df['time'].str.split('第').str[1].str.replace('季度', '').astype(int)\n",
    "\n",
    "    # 划分数据集\n",
    "    train_size = int(0.82 * len(df))\n",
    "    train_data = df.iloc[:train_size]\n",
    "    test_data = df.iloc[train_size:]\n",
    "\n",
    "    # 准备特征和目标变量\n",
    "    X_train = train_data[['Year', 'Quarter']]\n",
    "    y_train = train_data['values']\n",
    "    X_test = test_data[['Year', 'Quarter']]\n",
    "    y_test = test_data['values']\n",
    "\n",
    "    SearchAgents_no = 50  # 灰狼个数\n",
    "    T = 2  # 最大迭代次数\n",
    "    dim = 3  # 维度 需要优化两个变量 - depth lr es\n",
    "    lb = 1  # 最小值限制\n",
    "    ub = 50  # 最大值限制\n",
    "\n",
    "    print('----------------3.调用灰狼算法函数-----------------')\n",
    "    best_depth, best_lr, best_estimators, iterations, accuracy = sanitized_gwo(X_train, X_test, y_train, y_test,\n",
    "                                                                               SearchAgents_no, T,\n",
    "                                                                               dim,\n",
    "                                                                               lb, ub)\n",
    "\n",
    "    print('----------------4. 最优结果展示-----------------')\n",
    "    print(\"The best depth is \" + str(int(abs(best_depth))))  # 输出数据\n",
    "    print(\"The best lr is \" + str(int(abs(best_lr)) * 0.01))  # 输出数据\n",
    "    print(\"The best estimators is \" + str(int(abs(best_estimators)) * 100))  # 输出数据\n",
    "\n",
    "    print('----------------5. 应用优化后的最优参数值构建SVR回归模型-----------------')\n",
    "    # 建立XGB模型并训练\n",
    "    # XGB预测\n",
    "    model = xgb.XGBRegressor(max_depth=int(abs(best_depth)), learning_rate=int(abs(best_lr)) * 0.01,\n",
    "                             n_estimators=int(abs(best_estimators)) * 100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mape = np.mean(np.abs((y_pred - y_test) / y_test))\n",
    "    print('----------------模型评估-----------------')\n",
    "    # 模型评估\n",
    "    print('**************************输出测试集的模型评估指标结果*******************************')\n",
    "    print('XGB回归模型-最优参数-MAPE：', mape)\n",
    "\n",
    "\n",
    "    # 真实值与预测值比对图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "    plt.plot(df['time'], df['values'], label='真实数量', color='black')\n",
    "    last_five_time = df['time'].tail(4)\n",
    "    # plt.plot(last_five_time, y_test, label='预测数量', color='r')\n",
    "    plt.plot(last_five_time, y_pred, label='预测数量', color='r')\n",
    "    # plt.plot(range(len(y_test)), y_test, color=\"blue\", linewidth=1.5, linestyle=\"-\")  # 绘图\n",
    "    # plt.plot(range(len(y_pred)), y_pred, color=\"red\", linewidth=1.5, linestyle=\"-.\")  # 绘图\n",
    "    plt.legend(['真实值', '预测值'])  # 设置图例\n",
    "    plt.title(\"智能灰狼优化算法优化XGB回归模型真实值与预测值比对图\")  # 设置标题名称\n",
    "    plt.show()  # 显示图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375c5c6-52ef-4295-806d-f8777971630c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-agent (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
